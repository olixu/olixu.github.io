<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="keywords" content="Hexo Theme Redefine"><meta name="author" content="Oliver xu"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="canonical" href="https://blog.oliverxu.cn/2019/11/01/tensorflow-tutorial/"><meta name="robots" content="index,follow"><meta name="googlebot" content="index,follow"><meta name="revisit-after" content="1 days"><meta name="description" content="This tutorial will provide a brief overview of the core concepts and functionality of Tensorflow. This tutorial will cover the following:  What is Tensorflow How to input data How to perform computati"><meta property="og:type" content="article"><meta property="og:title" content="Tensorflow tutorial"><meta property="og:url" content="https://blog.oliverxu.cn/2019/11/01/Tensorflow-tutorial/index.html"><meta property="og:site_name" content="Oliver xu&#39;s Blog"><meta property="og:description" content="This tutorial will provide a brief overview of the core concepts and functionality of Tensorflow. This tutorial will cover the following:  What is Tensorflow How to input data How to perform computati"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://blog.oliverxu.cn/2019/11/01/Tensorflow-tutorial/output_41_1.png"><meta property="og:image" content="https://blog.oliverxu.cn/2019/11/01/Tensorflow-tutorial/output_45_2.png"><meta property="article:published_time" content="2019-11-01T19:18:54.000Z"><meta property="article:modified_time" content="2026-02-28T21:09:01.147Z"><meta property="article:author" content="Oliver xu"><meta property="article:tag" content="Tensorflow"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://blog.oliverxu.cn/2019/11/01/Tensorflow-tutorial/output_41_1.png"><link rel="icon" type="image/png" href="/images/avatar.jpeg" sizes="192x192"><link rel="apple-touch-icon" sizes="180x180" href="/images/avatar.jpeg"><meta name="theme-color" content="#A31F34"><link rel="shortcut icon" href="/images/avatar.jpeg"><title>Tensorflow tutorial | Oliver xu&#39;s Blog</title><link rel="stylesheet" href="/fonts/Chillax/chillax.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/assets/build/styles.css"><link rel="stylesheet" href="/fonts/GeistMono/geist-mono.css"><link rel="stylesheet" href="/fonts/Geist/geist.css"><script id="hexo-configurations">window.config={hostname:"blog.oliverxu.cn",root:"/",language:"zh-CN",path:"search.xml"},window.theme={articles:{style:{font_size:"16px",line_height:1.5,image_border_radius:"14px",image_alignment:"center",image_caption:!1,link_icon:!0,title_alignment:"left",headings_top_spacing:{h1:"3.2rem",h2:"2.4rem",h3:"1.9rem",h4:"1.6rem",h5:"1.4rem",h6:"1.3rem"}},word_count:{enable:!0,count:!0,min2read:!0},author_label:{enable:!1,auto:!1,list:[]},code_block:{copy:!0,style:"mac",highlight_theme:{light:"github",dark:"vs2015"},font:{enable:!1,family:null,url:null}},toc:{enable:!0,max_depth:3,number:!1,expand:!0,init_open:!0},copyright:{enable:!0,default:"cc_by_nc_sa"},lazyload:!0,recommendation:{enable:!1,title:"推荐阅读",limit:3,mobile_limit:2,placeholder:"/images/wallhaven-wqery6-light.webp",skip_dirs:[]}},colors:{primary:"#A31F34",secondary:null,default_mode:"light"},global:{fonts:{chinese:{enable:!1,family:null,url:null},english:{enable:!1,family:null,url:null},title:{enable:!1,family:null,url:null}},content_max_width:"1000px",sidebar_width:"210px",hover:{shadow:!0,scale:!1},scroll_progress:{bar:!0,percentage:!0},website_counter:{url:"https://cn.vercount.one/js",enable:!0,site_pv:!0,site_uv:!0,post_pv:!0},single_page:!0,preloader:{enable:!1,custom_message:null},open_graph:!0,google_analytics:{enable:!1,id:null}},home_banner:{enable:!0,style:"fixed",image:{light:"/images/xinjiang.webp",dark:"/images/xinjiang.webp"},title:"美好的事情即将发生",subtitle:{text:[],hitokoto:{enable:!1,show_author:!1,api:"https://v1.hitokoto.cn"},typing_speed:100,backing_speed:80,starting_delay:500,backing_delay:1500,loop:!0,smart_backspace:!0},text_color:{light:"#fff",dark:"#d1d1b6"},text_style:{title_size:"2.8rem",subtitle_size:"1.5rem",line_height:1.2},custom_font:{enable:!1,family:null,url:null},social_links:{enable:!0,style:"default",links:{github:"https://github.com/olixu",instagram:null,zhihu:null,twitter:null,email:null},qrs:{weixin:null}}},plugins:{feed:{enable:!1},aplayer:{enable:!1,type:"fixed",audios:[{name:null,artist:null,url:null,cover:null,lrc:null}]},mermaid:{enable:!1,version:"9.3.0"}},version:"2.7.3",navbar:{auto_hide:!1,color:{left:"#f78736",right:"#367df7",transparency:35},width:{home:"1200px",pages:"1000px"},links:{Home:{path:"/",icon:"fa-regular fa-house"},about:{path:"/about",icon:"fa fa-user"},tags:{path:"/tags",icon:"fa fa-tags"},archives:{path:"/archives",icon:"fa fa-archive"},album:{path:"/album",icon:"fa fa-images"}},search:{enable:!0,preload:!0}},page_templates:{friends_column:2,tags_style:"blur"},home:{sidebar:{enable:!0,position:"left",first_item:"menu",announcement:null,show_on_mobile:!0,links:null},article_date_format:"auto",excerpt_length:200,categories:{enable:!0,limit:3},tags:{enable:!0,limit:3}},footerStart:"2019/1/1 00:00:00"},window.lang_ago={second:"%s 秒前",minute:"%s 分钟前",hour:"%s 小时前",day:"%s 天前",week:"%s 周前",month:"%s 个月前",year:"%s 年前"},window.data={masonry:!1}</script><link rel="stylesheet" href="/fontawesome/fontawesome.min.css"><link rel="stylesheet" href="/fontawesome/brands.min.css"><link rel="stylesheet" href="/fontawesome/solid.min.css"><link rel="stylesheet" href="/fontawesome/regular.min.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div class="progress-bar-container"><span class="scroll-progress-bar"></span> <span class="pjax-progress-bar"></span></div><main class="page-container" id="swup"><div class="main-content-container flex flex-col justify-between min-h-dvh"><div class="main-content-header"><header class="navbar-container px-6 md:px-12"><div class="navbar-content transition-navbar"><div class="left"><a class="logo-title" href="/">Oliver xu&#39;s Blog</a></div><div class="right"><div class="desktop"><ul class="navbar-list"><li class="navbar-item"><a href="/"><i class="fa-regular fa-house fa-fw"></i> 首页</a></li><li class="navbar-item"><a href="/about"><i class="fa fa-user fa-fw"></i> 关于</a></li><li class="navbar-item"><a href="/tags"><i class="fa fa-tags fa-fw"></i> 标签</a></li><li class="navbar-item"><a href="/archives"><i class="fa fa-archive fa-fw"></i> 归档</a></li><li class="navbar-item"><a href="/album"><i class="fa fa-images fa-fw"></i> ALBUM</a></li><li class="navbar-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></li></ul></div><div class="mobile"><div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></div><div class="icon-item navbar-bar"><div class="navbar-bar-middle"></div></div></div></div></div><div class="navbar-drawer h-dvh w-full absolute top-0 left-0 bg-background-color flex flex-col justify-between"><ul class="drawer-navbar-list flex flex-col px-4 justify-center items-start"><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" href="/"><span>首页 </span><i class="fa-regular fa-house fa-sm fa-fw"></i></a></li><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" href="/about"><span>关于 </span><i class="fa fa-user fa-sm fa-fw"></i></a></li><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" href="/tags"><span>标签 </span><i class="fa fa-tags fa-sm fa-fw"></i></a></li><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" href="/archives"><span>归档 </span><i class="fa fa-archive fa-sm fa-fw"></i></a></li><li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full"><a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full" href="/album"><span>ALBUM </span><i class="fa fa-images fa-sm fa-fw"></i></a></li></ul><div class="statistics flex justify-around my-2.5"><a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/tags"><div class="number text-2xl sm:text-xl text-second-text-color font-semibold">45</div><div class="label text-third-text-color text-sm">标签</div></a><a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/categories"><div class="number text-2xl sm:text-xl text-second-text-color font-semibold">42</div><div class="label text-third-text-color text-sm">分类</div></a><a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/archives"><div class="number text-2xl sm:text-xl text-second-text-color font-semibold">94</div><div class="label text-third-text-color text-sm">文章</div></a></div></div><div class="window-mask"></div></header></div><div class="main-content-body"><div class="main-content"><div class="post-page-container flex relative justify-between box-border w-full h-full"><div class="article-content-container"><div class="article-title relative w-full"><div class="w-full flex items-center pt-6 justify-start"><h1 class="article-title-regular text-second-text-color tracking-tight text-4xl md:text-6xl font-semibold px-2 sm:px-6 md:px-8 py-3">Tensorflow tutorial</h1></div></div><div class="article-header flex flex-row gap-2 items-center px-2 sm:px-6 md:px-8"><div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]"><img src="/images/avatar.jpeg"></div><div class="info flex flex-col justify-between"><div class="author flex items-center"><span class="name text-default-text-color text-lg font-semibold">Oliver xu</span></div><div class="meta-info"><div class="article-meta-info"><span class="article-date article-meta-item"><i class="fa-regular fa-pen-fancy"></i>&nbsp; <span class="desktop">2019-11-01 19:18:54</span> <span class="mobile">2019-11-01 19:18:54</span> <span class="hover-info">创建</span> </span><span class="article-date article-meta-item"><i class="fa-regular fa-wrench"></i>&nbsp; <span class="desktop">2026-02-28 21:09:01</span> <span class="mobile">2026-02-28 21:09:01</span> <span class="hover-info">更新</span> </span><span class="article-categories article-meta-item"><i class="fa-regular fa-folders"></i>&nbsp;<ul><li><a href="/categories/Tensorflow/">Tensorflow</a>&nbsp;</li></ul></span><span class="article-tags article-meta-item"><i class="fa-regular fa-tags"></i>&nbsp;<ul><li><a href="/tags/Tensorflow/">Tensorflow</a>&nbsp;</li></ul></span><span class="article-wordcount article-meta-item"><i class="fa-regular fa-typewriter"></i>&nbsp;<span>2.8k 字</span> </span><span class="article-min2read article-meta-item"><i class="fa-regular fa-clock"></i>&nbsp;<span>17 分钟</span> </span><span class="article-pv article-meta-item"><i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span></span></div></div></div></div><div class="article-content markdown-body px-2 sm:px-6 md:px-8 pb-8"><p>This tutorial will provide a brief overview of the core concepts and functionality of Tensorflow. This tutorial will cover the following:</p><ol start="0" type="1"><li>What is Tensorflow</li><li>How to input data</li><li>How to perform computations</li><li>How to create variables</li><li>How to train a neural network for a simple regression problem</li><li>Tips and tricks <span id="more"></span></li></ol><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.cm <span class="keyword">as</span> cm</span><br><span class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span> mpatches</span><br></pre></td></tr></table></figure></div><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tf_reset</span>():</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        sess.close()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    tf.reset_default_graph()</span><br><span class="line">    <span class="keyword">return</span> tf.Session()</span><br></pre></td></tr></table></figure></div><h1 id="what-is-tensorflow">0. What is Tensorflow</h1><p>Tensorflow is a framework to define a series of computations. You define inputs, what operations should be performed, and then Tensorflow will compute the outputs for you.</p><p>Below is a simple high-level example:</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create the session you&#x27;ll work in</span></span><br><span class="line"><span class="comment"># you can think of this as a &quot;blank piece of paper&quot; that you&#x27;ll be writing math on</span></span><br><span class="line">sess = tf_reset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># define your inputs</span></span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># do some operations</span></span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the result</span></span><br><span class="line">c_run = sess.run(c)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;c = &#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(c_run))</span><br></pre></td></tr></table></figure></div><pre><code>c = 3.0</code></pre><h1 id="how-to-input-data">1. How to input data</h1><p>Tensorflow has multiple ways for you to input data. One way is to have the inputs be constants:</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sess = tf_reset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># define your inputs</span></span><br><span class="line">a = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># do some operations</span></span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the result</span></span><br><span class="line">c_run = sess.run(c)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;c = &#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(c_run))</span><br></pre></td></tr></table></figure></div><pre><code>c = 3.0</code></pre><p>However, having our inputs be constants is inflexible. We want to be able to change what data we input at runtime. We can do this using placeholders:</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sess = tf_reset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># define your inputs</span></span><br><span class="line">a = tf.placeholder(dtype=tf.float32, shape=[<span class="number">1</span>], name=<span class="string">&#x27;a_placeholder&#x27;</span>)</span><br><span class="line">b = tf.placeholder(dtype=tf.float32, shape=[<span class="number">1</span>], name=<span class="string">&#x27;b_placeholder&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># do some operations</span></span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the result</span></span><br><span class="line">c0_run = sess.run(c, feed_dict=&#123;a: [<span class="number">1.0</span>], b: [<span class="number">2.0</span>]&#125;)</span><br><span class="line">c1_run = sess.run(c, feed_dict=&#123;a: [<span class="number">2.0</span>], b: [<span class="number">4.0</span>]&#125;)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;c0 = &#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(c0_run))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;c1 = &#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(c1_run))</span><br></pre></td></tr></table></figure></div><pre><code>c0 = [3.]
c1 = [6.]</code></pre><p>But what if we don't know the size of our input beforehand? One dimension of a tensor is allowed to be 'None', which means it can be variable sized:</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sess = tf_reset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># inputs</span></span><br><span class="line">a = tf.placeholder(dtype=tf.float32, shape=[<span class="literal">None</span>], name=<span class="string">&#x27;a_placeholder&#x27;</span>)</span><br><span class="line">b = tf.placeholder(dtype=tf.float32, shape=[<span class="literal">None</span>], name=<span class="string">&#x27;b_placeholder&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># do some operations</span></span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># get outputs</span></span><br><span class="line">c0_run = sess.run(c, feed_dict=&#123;a: [<span class="number">1.0</span>], b: [<span class="number">2.0</span>]&#125;)</span><br><span class="line">c1_run = sess.run(c, feed_dict=&#123;a: [<span class="number">1.0</span>, <span class="number">2.0</span>], b: [<span class="number">2.0</span>, <span class="number">4.0</span>]&#125;)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;a shape: &#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(a.get_shape()))</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b shape: &#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(b.get_shape()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;c0 = &#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(c0_run))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;c1 = &#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(c1_run))</span><br></pre></td></tr></table></figure></div><pre><code>Tensor(&quot;a_placeholder:0&quot;, shape=(?,), dtype=float32)
a shape: (?,)
Tensor(&quot;b_placeholder:0&quot;, shape=(?,), dtype=float32)
b shape: (?,)
c0 = [3.]
c1 = [3. 6.]</code></pre><h1 id="how-to-perform-computations">2. How to perform computations</h1><p>Now that we can input data, we want to perform useful computations on the data.</p><p>First, let's create some data to work with:</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sess = tf_reset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># inputs</span></span><br><span class="line">a = tf.constant([[-<span class="number">1.</span>], [-<span class="number">2.</span>], [-<span class="number">3.</span>]], dtype=tf.float32)</span><br><span class="line">b = tf.constant([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>]], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">a_run, b_run = sess.run([a, b])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;a:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(a_run))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(b_run))</span><br></pre></td></tr></table></figure></div><pre><code>a:
[[-1.]
 [-2.]
 [-3.]]
b:
[[1. 2. 3.]]</code></pre><p>We can do simple operations, such as addition:</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c = b + b</span><br><span class="line"></span><br><span class="line">c_run = sess.run(c)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(b_run))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;c:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(c_run))</span><br></pre></td></tr></table></figure></div><pre><code>b:
[[1. 2. 3.]]
c:
[[2. 4. 6.]]</code></pre><p>Be careful about the dimensions of the tensors, some operations may work even when you think they shouldn't...</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line">c_run = sess.run(c)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;a:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(a_run))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(b_run))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;c:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(c_run))</span><br></pre></td></tr></table></figure></div><pre><code>a:
[[-1.]
 [-2.]
 [-3.]]
b:
[[1. 2. 3.]]
c:
[[ 0.  1.  2.]
 [-1.  0.  1.]
 [-2. -1.  0.]]</code></pre><p>Also, some operations may be different than what you expect:</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">c_elementwise = a * b</span><br><span class="line">c_matmul = tf.matmul(b, a)</span><br><span class="line"></span><br><span class="line">c_elementwise_run, c_matmul_run = sess.run([c_elementwise, c_matmul])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;a:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(a_run))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(b_run))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;c_elementwise:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(c_elementwise_run))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;c_matmul: \n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(c_matmul_run))</span><br></pre></td></tr></table></figure></div><pre><code>a:
[[-1.]
 [-2.]
 [-3.]]
b:
[[1. 2. 3.]]
c_elementwise:
[[-1. -2. -3.]
 [-2. -4. -6.]
 [-3. -6. -9.]]
c_matmul: 
[[-14.]]</code></pre><p>Operations can be chained together:</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># operations can be chained together</span></span><br><span class="line">c0 = b + b</span><br><span class="line">c1 = c0 + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">c0_run, c1_run = sess.run([c0, c1])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(b_run))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;c0:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(c0_run))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;c1:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(c1_run))</span><br></pre></td></tr></table></figure></div><pre><code>b:
[[1. 2. 3.]]
c0:
[[2. 4. 6.]]
c1:
[[3. 5. 7.]]</code></pre><p>Finally, Tensorflow has many useful built-in operations:</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c = tf.reduce_mean(b)</span><br><span class="line"></span><br><span class="line">c_run = sess.run(c)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(b_run))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;c:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(c_run))</span><br></pre></td></tr></table></figure></div><pre><code>b:
[[1. 2. 3.]]
c:
2.0</code></pre><h1 id="how-to-create-variables">3. How to create variables</h1><p>Now that we can input data and perform computations, we want some of these operations to involve variables that are free parameters, and can be trained using an optimizer (e.g., gradient descent).</p><p>First, let's create some data to work with:</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sess = tf_reset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># inputs</span></span><br><span class="line">b = tf.constant([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>]], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">b_run = sess.run(b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(b_run))</span><br></pre></td></tr></table></figure></div><pre><code>b:
[[1. 2. 3.]]</code></pre><p>We'll now create a variable</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">var_init_value = [[<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]]</span><br><span class="line">var = tf.get_variable(name=<span class="string">&#x27;myvar&#x27;</span>,</span><br><span class="line">                      shape=[<span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">                      dtype=tf.float32,</span><br><span class="line">                      initializer=tf.constant_initializer(var_init_value))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(var)</span><br></pre></td></tr></table></figure></div><pre><code>&lt;tf.Variable &#39;myvar:0&#39; shape=(1, 3) dtype=float32_ref&gt;</code></pre><p>and check that it's been added to Tensorflow's variables list:</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tf.global_variables())</span><br></pre></td></tr></table></figure></div><pre><code>[&lt;tf.Variable &#39;myvar:0&#39; shape=(1, 3) dtype=float32_ref&gt;]</code></pre><p>We can do operations with the variable just like any other tensor:</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># can do operations</span></span><br><span class="line">c = b + var</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(var)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br></pre></td></tr></table></figure></div><pre><code>Tensor(&quot;Const:0&quot;, shape=(1, 3), dtype=float32)
&lt;tf.Variable &#39;myvar:0&#39; shape=(1, 3) dtype=float32_ref&gt;
Tensor(&quot;add:0&quot;, shape=(1, 3), dtype=float32)</code></pre><p>Before we can run any of these operations, we must first initalize the variables</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line">sess.run(init_op)</span><br></pre></td></tr></table></figure></div><p>and then we can run the operations just as we normally would.</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c_run = sess.run(c)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(b_run))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;var:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(var_init_value))http://rail.eecs.berkeley.edu/deeprlcourse-fa18/static/misc/TF_lecture.ipynb</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;c:\n&#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(c_run))</span><br></pre></td></tr></table></figure></div><pre><code>b:
[[1. 2. 3.]]
var:
[[2.0, 4.0, 6.0]]
c:
[[3. 6. 9.]]</code></pre><p>So far we haven't said yet how to optimize these variables. We'll cover that next in the context of an example.</p><h1 id="how-to-train-a-neural-network-for-a-simple-regression-problem">4. How to train a neural network for a simple regression problem</h1><p>We've discussed how to input data, perform operations, and create variables. We'll now show how to combine all of these---with some minor additions---to train a neural network on a simple regression problem.</p><p>First, we'll create data for a 1-dimensional regression problem:</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generate the data</span></span><br><span class="line">inputs = np.linspace(-<span class="number">2</span>*np.pi, <span class="number">2</span>*np.pi, <span class="number">10000</span>)[:, <span class="literal">None</span>]</span><br><span class="line">outputs = np.sin(inputs) + <span class="number">0.05</span> * np.random.normal(size=[<span class="built_in">len</span>(inputs),<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">plt.scatter(inputs[:, <span class="number">0</span>], outputs[:, <span class="number">0</span>], s=<span class="number">0.1</span>, color=<span class="string">&#x27;k&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br></pre></td></tr></table></figure></div><pre><code>&lt;matplotlib.collections.PathCollection at 0x7ffa1bed23c8&gt;</code></pre><img lazyload src="/images/loading.svg" data-src="/2019/11/01/Tensorflow-tutorial/output_41_1.png" title="png"><p>The below code creates the inputs, variables, neural network operations, mean-squared-error loss, gradient descent optimizer, and runs the optimizer using minibatches of the data.</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">sess = tf_reset()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_model</span>():</span><br><span class="line">    <span class="comment"># create inputs</span></span><br><span class="line">    input_ph = tf.placeholder(dtype=tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">    output_ph = tf.placeholder(dtype=tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create variables</span></span><br><span class="line">    W0 = tf.get_variable(name=<span class="string">&#x27;W0&#x27;</span>, shape=[<span class="number">1</span>, <span class="number">20</span>], initializer=tf.contrib.layers.xavier_initializer())</span><br><span class="line">    W1 = tf.get_variable(name=<span class="string">&#x27;W1&#x27;</span>, shape=[<span class="number">20</span>, <span class="number">20</span>], initializer=tf.contrib.layers.xavier_initializer())</span><br><span class="line">    W2 = tf.get_variable(name=<span class="string">&#x27;W2&#x27;</span>, shape=[<span class="number">20</span>, <span class="number">1</span>], initializer=tf.contrib.layers.xavier_initializer())</span><br><span class="line"></span><br><span class="line">    b0 = tf.get_variable(name=<span class="string">&#x27;b0&#x27;</span>, shape=[<span class="number">20</span>], initializer=tf.constant_initializer(<span class="number">0.</span>))</span><br><span class="line">    b1 = tf.get_variable(name=<span class="string">&#x27;b1&#x27;</span>, shape=[<span class="number">20</span>], initializer=tf.constant_initializer(<span class="number">0.</span>))</span><br><span class="line">    b2 = tf.get_variable(name=<span class="string">&#x27;b2&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">0.</span>))</span><br><span class="line"></span><br><span class="line">    weights = [W0, W1, W2]</span><br><span class="line">    biases = [b0, b1, b2]</span><br><span class="line">    activations = [tf.nn.relu, tf.nn.relu, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># create computation graph</span></span><br><span class="line">    layer = input_ph</span><br><span class="line">    <span class="keyword">for</span> W, b, activation <span class="keyword">in</span> <span class="built_in">zip</span>(weights, biases, activations):</span><br><span class="line">        layer = tf.matmul(layer, W) + b</span><br><span class="line">        <span class="keyword">if</span> activation <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            layer = activation(layer)</span><br><span class="line">    output_pred = layer</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> input_ph, output_ph, output_pred</span><br><span class="line">    </span><br><span class="line">input_ph, output_ph, output_pred = create_model()</span><br><span class="line">    </span><br><span class="line"><span class="comment"># create loss</span></span><br><span class="line">mse = tf.reduce_mean(<span class="number">0.5</span> * tf.square(output_pred - output_ph))</span><br><span class="line"></span><br><span class="line"><span class="comment"># create optimizer</span></span><br><span class="line">opt = tf.train.AdamOptimizer().minimize(mse)</span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize variables</span></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line"><span class="comment"># create saver to save model variables</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line"><span class="comment"># run training</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line"><span class="keyword">for</span> training_step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10000</span>):</span><br><span class="line">    <span class="comment"># get a random subset of the training data</span></span><br><span class="line">    indices = np.random.randint(low=<span class="number">0</span>, high=<span class="built_in">len</span>(inputs), size=batch_size)</span><br><span class="line">    input_batch = inputs[indices]</span><br><span class="line">    output_batch = outputs[indices]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># run the optimizer and get the mse</span></span><br><span class="line">    _, mse_run = sess.run([opt, mse], feed_dict=&#123;input_ph: input_batch, output_ph: output_batch&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># print the mse every so often</span></span><br><span class="line">    <span class="keyword">if</span> training_step % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;&#123;0:04d&#125; mse: &#123;1:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(training_step, mse_run))</span><br><span class="line">        saver.save(sess, <span class="string">&#x27;./tmp/model.ckpt&#x27;</span>)</span><br></pre></td></tr></table></figure></div><pre><code>0000 mse: 0.428
1000 mse: 0.067
2000 mse: 0.027
3000 mse: 0.016
4000 mse: 0.006
5000 mse: 0.002
6000 mse: 0.002
7000 mse: 0.001
8000 mse: 0.002
9000 mse: 0.001</code></pre><p>Now that the neural network is trained, we can use it to make predictions:</p><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sess = tf_reset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># create the model</span></span><br><span class="line">input_ph, output_ph, output_pred = create_model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># restore the saved model</span></span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">saver.restore(sess, <span class="string">&quot;./tmp/model.ckpt&quot;</span>)</span><br><span class="line"></span><br><span class="line">output_pred_run = sess.run(output_pred, feed_dict=&#123;input_ph: inputs&#125;)</span><br><span class="line"></span><br><span class="line">plt.scatter(inputs[:, <span class="number">0</span>], outputs[:, <span class="number">0</span>], c=<span class="string">&#x27;k&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, s=<span class="number">0.1</span>)</span><br><span class="line">plt.scatter(inputs[:, <span class="number">0</span>], output_pred_run[:, <span class="number">0</span>], c=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, s=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure></div><pre><code>INFO:tensorflow:Restoring parameters from /tmp/model.ckpt





&lt;matplotlib.collections.PathCollection at 0x7ff9dc62a550&gt;</code></pre><img lazyload src="/images/loading.svg" data-src="/2019/11/01/Tensorflow-tutorial/output_45_2.png" title="png"><p>Not so hard after all! There is much more functionality to Tensorflow besides what we've covered, but you now know the basics.</p><h1 id="tips-and-tricks">5. Tips and tricks</h1><h5 id="a-check-your-dimensions">(a) Check your dimensions</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example of &quot;surprising&quot; resulting dimensions due to broadcasting</span></span><br><span class="line">a = tf.constant(np.random.random((<span class="number">4</span>, <span class="number">1</span>)))</span><br><span class="line">b = tf.constant(np.random.random((<span class="number">1</span>, <span class="number">4</span>)))</span><br><span class="line">c = a * b</span><br><span class="line"><span class="keyword">assert</span> c.get_shape() == (<span class="number">4</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure></div><h5 id="b-check-what-variables-have-been-created">(b) Check what variables have been created</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sess = tf_reset()</span><br><span class="line">a = tf.get_variable(<span class="string">&#x27;I_am_a_variable&#x27;</span>, shape=[<span class="number">4</span>, <span class="number">6</span>])</span><br><span class="line">b = tf.get_variable(<span class="string">&#x27;I_am_a_variable_too&#x27;</span>, shape=[<span class="number">2</span>, <span class="number">7</span>])</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> tf.global_variables():</span><br><span class="line">    <span class="built_in">print</span>(var.name)</span><br></pre></td></tr></table></figure></div><pre><code>I_am_a_variable:0
I_am_a_variable_too:0</code></pre><h5 id="c-look-at-the-tensorflow-api-or-open-up-a-python-terminal-and-investigate">(c) Look at the <a class="link" target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/">tensorflow API <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>, or open up a python terminal and investigate!</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">help</span>(tf.reduce_mean)</span><br></pre></td></tr></table></figure></div><pre><code>Help on function reduce_mean in module tensorflow.python.ops.math_ops:

reduce_mean(input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None, keep_dims=None)
    Computes the mean of elements across dimensions of a tensor. (deprecated arguments)
    
    SOME ARGUMENTS ARE DEPRECATED. They will be removed in a future version.
    Instructions for updating:
    keep_dims is deprecated, use keepdims instead
    
    Reduces `input_tensor` along the dimensions given in `axis`.
    Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each
    entry in `axis`. If `keepdims` is true, the reduced dimensions
    are retained with length 1.
    
    If `axis` is None, all dimensions are reduced, and a
    tensor with a single element is returned.
    
    For example:
    
    <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">1.</span>, <span class="number">1.</span>], [<span class="number">2.</span>, <span class="number">2.</span>]])</span><br><span class="line">tf.reduce_mean(x)  <span class="comment"># 1.5</span></span><br><span class="line">tf.reduce_mean(x, <span class="number">0</span>)  <span class="comment"># [1.5, 1.5]</span></span><br><span class="line">tf.reduce_mean(x, <span class="number">1</span>)  <span class="comment"># [1.,  2.]</span></span><br></pre></td></tr></table></figure></div>
    
    Args:
      input_tensor: The tensor to reduce. Should have numeric type.
      axis: The dimensions to reduce. If `None` (the default),
        reduces all dimensions. Must be in the range
        `[-rank(input_tensor), rank(input_tensor))`.
      keepdims: If true, retains reduced dimensions with length 1.
      name: A name for the operation (optional).
      reduction_indices: The old (deprecated) name for axis.
      keep_dims: Deprecated alias for `keepdims`.
    
    Returns:
      The reduced tensor.
    
    @compatibility(numpy)
    Equivalent to np.mean
    
    Please note that `np.mean` has a `dtype` parameter that could be used to
    specify the output type. By default this is `dtype=float64`. On the other
    hand, `tf.reduce_mean` has an aggressive type inference from `input_tensor`,
    for example:
    
    <div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">tf.reduce_mean(x)  <span class="comment"># 0</span></span><br><span class="line">y = tf.constant([<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>])</span><br><span class="line">tf.reduce_mean(y)  <span class="comment"># 0.5</span></span><br></pre></td></tr></table></figure></div>
    
    @end_compatibility</code></pre><h5 id="d-tensorflow-has-some-built-in-layers-to-simplify-your-code.">(d) Tensorflow has some built-in layers to simplify your code.</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">help</span>(tf.contrib.layers.fully_connected)</span><br></pre></td></tr></table></figure></div><pre><code>Help on function fully_connected in module tensorflow.contrib.layers.python.layers.layers:

fully_connected(inputs, num_outputs, activation_fn=&lt;function relu at 0x7ffa20054c80&gt;, normalizer_fn=None, normalizer_params=None, weights_initializer=&lt;function variance_scaling_initializer.&lt;locals&gt;._initializer at 0x7ff9f2ecd158&gt;, weights_regularizer=None, biases_initializer=&lt;tensorflow.python.ops.init_ops.Zeros object at 0x7ff9f2ecc780&gt;, biases_regularizer=None, reuse=None, variables_collections=None, outputs_collections=None, trainable=True, scope=None)
    Adds a fully connected layer.
    
    `fully_connected` creates a variable called `weights`, representing a fully
    connected weight matrix, which is multiplied by the `inputs` to produce a
    `Tensor` of hidden units. If a `normalizer_fn` is provided (such as
    `batch_norm`), it is then applied. Otherwise, if `normalizer_fn` is
    None and a `biases_initializer` is provided then a `biases` variable would be
    created and added the hidden units. Finally, if `activation_fn` is not `None`,
    it is applied to the hidden units as well.
    
    Note: that if `inputs` have a rank greater than 2, then `inputs` is flattened
    prior to the initial matrix multiply by `weights`.
    
    Args:
      inputs: A tensor of at least rank 2 and static value for the last dimension;
        i.e. `[batch_size, depth]`, `[None, None, None, channels]`.
      num_outputs: Integer or long, the number of output units in the layer.
      activation_fn: Activation function. The default value is a ReLU function.
        Explicitly set it to None to skip it and maintain a linear activation.
      normalizer_fn: Normalization function to use instead of `biases`. If
        `normalizer_fn` is provided then `biases_initializer` and
        `biases_regularizer` are ignored and `biases` are not created nor added.
        default set to None for no normalizer function
      normalizer_params: Normalization function parameters.
      weights_initializer: An initializer for the weights.
      weights_regularizer: Optional regularizer for the weights.
      biases_initializer: An initializer for the biases. If None skip biases.
      biases_regularizer: Optional regularizer for the biases.
      reuse: Whether or not the layer and its variables should be reused. To be
        able to reuse the layer scope must be given.
      variables_collections: Optional list of collections for all the variables or
        a dictionary containing a different list of collections per variable.
      outputs_collections: Collection to add the outputs.
      trainable: If `True` also add variables to the graph collection
        `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).
      scope: Optional scope for variable_scope.
    
    Returns:
       The tensor variable representing the result of the series of operations.
    
    Raises:
      ValueError: If x has rank less than 2 or if its last dimension is not set.</code></pre><h5 id="e-use-variable-scope-to-keep-your-variables-organized.">(e) Use <a class="link" target="_blank" rel="noopener" href="https://www.tensorflow.org/guide/variables#sharing_variables">variable scope <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> to keep your variables organized.</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">sess = tf_reset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># create variables</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;layer_0&#x27;</span>):</span><br><span class="line">    W0 = tf.get_variable(name=<span class="string">&#x27;W0&#x27;</span>, shape=[<span class="number">1</span>, <span class="number">20</span>], initializer=tf.contrib.layers.xavier_initializer())</span><br><span class="line">    b0 = tf.get_variable(name=<span class="string">&#x27;b0&#x27;</span>, shape=[<span class="number">20</span>], initializer=tf.constant_initializer(<span class="number">0.</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;layer_1&#x27;</span>):</span><br><span class="line">    W1 = tf.get_variable(name=<span class="string">&#x27;W1&#x27;</span>, shape=[<span class="number">20</span>, <span class="number">20</span>], initializer=tf.contrib.layers.xavier_initializer())</span><br><span class="line">    b1 = tf.get_variable(name=<span class="string">&#x27;b1&#x27;</span>, shape=[<span class="number">20</span>], initializer=tf.constant_initializer(<span class="number">0.</span>))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;layer_2&#x27;</span>):</span><br><span class="line">    W2 = tf.get_variable(name=<span class="string">&#x27;W2&#x27;</span>, shape=[<span class="number">20</span>, <span class="number">1</span>], initializer=tf.contrib.layers.xavier_initializer())</span><br><span class="line">    b2 = tf.get_variable(name=<span class="string">&#x27;b2&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">0.</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the variables</span></span><br><span class="line">var_names = <span class="built_in">sorted</span>([v.name <span class="keyword">for</span> v <span class="keyword">in</span> tf.global_variables()])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n&#x27;</span>.join(var_names))</span><br></pre></td></tr></table></figure></div><pre><code>layer_0/W0:0
layer_0/b0:0
layer_1/W1:0
layer_1/b1:0
layer_2/W2:0
layer_2/b2:0</code></pre><h5 id="f-you-can-specify-which-gpu-you-want-to-use-and-how-much-memory-you-want-to-use">(f) You can specify which GPU you want to use and how much memory you want to use</h5><div class="highlight-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">gpu_device = <span class="number">0</span></span><br><span class="line">gpu_frac = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># make only one of the GPUs visible</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="built_in">str</span>(gpu_device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># only use part of the GPU memory</span></span><br><span class="line">gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_frac)</span><br><span class="line">config = tf.ConfigProto(gpu_options=gpu_options)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create the session</span></span><br><span class="line">tf_sess = tf.Session(graph=tf.Graph(), config=config)</span><br></pre></td></tr></table></figure></div><h5 id="g-you-can-use-tensorboard-to-visualize-and-monitor-the-training-process.">(g) You can use <a class="link" target="_blank" rel="noopener" href="https://www.tensorflow.org/guide/summaries_and_tensorboard">tensorboard <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> to visualize and monitor the training process.</h5></div><div class="post-copyright-info w-full my-8 px-2 sm:px-6 md:px-8"><div class="article-copyright-info-container"><ul><li><strong>标题:</strong> Tensorflow tutorial</li><li><strong>作者:</strong> Oliver xu</li><li><strong>创建于 :</strong> 2019-11-01 19:18:54</li><li><strong>更新于 :</strong> 2026-02-28 21:09:01</li><li><strong>链接:</strong> https://blog.oliverxu.cn/2019/11/01/Tensorflow-tutorial/</li><li><strong>版权声明: </strong>本文章采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0">CC BY-NC-SA 4.0</a> 进行许可。</li></ul></div></div><ul class="post-tags-box text-lg mt-1.5 flex-wrap justify-center flex md:hidden"><li class="tag-item mx-0.5"><a href="/tags/Tensorflow/">#Tensorflow</a>&nbsp;</li></ul><div class="article-nav my-8 flex justify-between items-center px-2 sm:px-6 md:px-8"><div class="article-prev border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2"><a class="prev" rel="prev" href="/2019/11/05/Mysql-Command/"><span class="left arrow-icon flex justify-center items-center"><i class="fa-solid fa-chevron-left"></i> </span><span class="title flex justify-center items-center"><span class="post-nav-title-item">Mysql Command</span> <span class="post-nav-item">上一篇</span></span></a></div><div class="article-next border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2"><a class="next" rel="next" href="/2019/10/25/Play-with-CartPole/"><span class="title flex justify-center items-center"><span class="post-nav-title-item">Play with CartPole</span> <span class="post-nav-item">下一篇</span> </span><span class="right arrow-icon flex justify-center items-center"><i class="fa-solid fa-chevron-right"></i></span></a></div></div><div class="comment-container px-2 sm:px-6 md:px-8 pb-8"><div class="comments-container mt-10 w-full"><div id="comment-anchor" class="w-full h-2.5"></div><div class="comment-area-title w-full my-1.5 md:my-2.5 text-xl md:text-3xl font-bold">评论</div><div id="gitalk-container"></div><script data-swup-reload-script src="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js"></script><script data-swup-reload-script>function loadGitalk(){let e=decodeURI(location.pathname);e.length>50&&(e=e.substring(0,47)+"...");try{Gitalk&&new Gitalk({clientID:"e11777414ce827fd8ec2",clientSecret:"c572c4ca1c0dfdfaf2b4f2195aa903185e2dfeea",repo:"blog-comment",owner:"olixu",admin:["olixu"],id:e,language:"zh-CN",proxy:"https://strong-caramel-969805.netlify.app/github_access_token"}).render("gitalk-container")}catch(e){window.Gitalk=null}}{const e=setTimeout(()=>{loadGitalk(),clearTimeout(e)},1e3)}</script></div></div></div><div class="toc-content-container"><div class="post-toc-wrap"><div class="post-toc"><div class="toc-title">目录</div><div class="page-title">Tensorflow tutorial</div><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#what-is-tensorflow"><span class="nav-text">0. What is Tensorflow</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#how-to-input-data"><span class="nav-text">1. How to input data</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#how-to-perform-computations"><span class="nav-text">2. How to perform computations</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#how-to-create-variables"><span class="nav-text">3. How to create variables</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#how-to-train-a-neural-network-for-a-simple-regression-problem"><span class="nav-text">4. How to train a neural network for a simple regression problem</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#tips-and-tricks"><span class="nav-text">5. Tips and tricks</span></a></li></ol></div></div></div></div></div></div><div class="main-content-footer"><footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color"><div class="info-container py-3 text-center"><div class="text-center">&copy; <span>2019</span> - 2026&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration:0.5s;color:#f54545"></i>&nbsp;&nbsp;<a href="/">Oliver xu</a><p class="post-count space-x-0.5"><span>共撰写了 94 篇文章 </span><span>共 123k 字</span></p></div><script data-swup-reload-script src="https://cn.vercount.one/js"></script><div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right"><span id="busuanzi_container_site_uv" class="lg:!block"><span class="text-sm">访问人数</span> <span id="busuanzi_value_site_uv"></span> </span><span id="busuanzi_container_site_pv" class="lg:!block"><span class="text-sm">总访问量</span> <span id="busuanzi_value_site_pv"></span></span></div><div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left"><span class="lg:block text-sm">由 <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a> 驱动</span> <span class="text-sm lg:block">主题&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.7.3</a></span></div><div class="icp-info my-1"><a target="_blank" rel="nofollow" href="https://beian.miit.gov.cn">苏ICP备17055396号</a></div><div>博客已运行 <span class="odometer" id="runtime_days"></span> 天 <span class="odometer" id="runtime_hours"></span> 小时 <span class="odometer" id="runtime_minutes"></span> 分钟 <span class="odometer" id="runtime_seconds"></span> 秒</div><script data-swup-reload-script>try{function odometer_init(){document.querySelectorAll(".odometer").forEach(e=>{new Odometer({el:e,format:"( ddd).dd",duration:200})})}odometer_init()}catch(e){}</script></div></footer></div></div><div class="post-tools"><div class="post-tools-container"><ul class="article-tools-list"><li class="right-bottom-tools page-aside-toggle"><i class="fa-regular fa-outdent"></i></li><li class="go-comment"><i class="fa-regular fa-comments"></i></li></ul></div></div><div class="right-side-tools-container"><div class="side-tools-container"><ul class="hidden-tools-list"><li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center"><i class="fa-regular fa-magnifying-glass-plus"></i></li><li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center"><i class="fa-regular fa-magnifying-glass-minus"></i></li><li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center"><i class="fa-regular fa-moon"></i></li><li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center"><i class="fa-regular fa-arrow-down"></i></li></ul><ul class="visible-tools-list"><li class="right-bottom-tools toggle-tools-list flex justify-center items-center"><i class="fa-regular fa-cog fa-spin"></i></li><li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center"><i class="arrow-up fas fa-arrow-up"></i> <span class="percent"></span></li></ul></div></div><div class="image-viewer-container"><img src=""></div><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-input-field-pre"><i class="fa-solid fa-keyboard"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="off" placeholder="站内搜索您需要的内容..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa-solid fa-times"></i></span></div><div id="search-result"><div id="no-result"><i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i></div></div></div></div></main><script src="/js/libs/Swup.min.js"></script><script src="/js/libs/SwupSlideTheme.min.js"></script><script src="/js/libs/SwupScriptsPlugin.min.js"></script><script src="/js/libs/SwupProgressPlugin.min.js"></script><script src="/js/libs/SwupScrollPlugin.min.js"></script><script src="/js/libs/SwupPreloadPlugin.min.js"></script><script>const swup=new Swup({plugins:[new SwupScriptsPlugin({optin:!0}),new SwupProgressPlugin,new SwupScrollPlugin({offset:80}),new SwupSlideTheme({mainElement:".main-content-body"}),new SwupPreloadPlugin],containers:["#swup"]})</script><script src="/js/tools/imageViewer.js" type="module"></script><script src="/js/utils.js" type="module"></script><script src="/js/main.js" type="module"></script><script src="/js/layouts/navbarShrink.js" type="module"></script><script src="/js/tools/scrollTopBottom.js" type="module"></script><script src="/js/tools/lightDarkSwitch.js" type="module"></script><script src="/js/layouts/categoryList.js" type="module"></script><script src="/js/tools/localSearch.js" type="module"></script><script src="/js/tools/codeBlock.js" type="module"></script><script src="/js/layouts/lazyload.js" type="module"></script><script src="/js/tools/runtime.js"></script><script src="/js/libs/odometer.min.js"></script><link rel="stylesheet" href="/assets/odometer-theme-minimal.css"><script src="/js/libs/Typed.min.js"></script><script src="/js/plugins/typed.js" type="module"></script><script src="/js/libs/anime.min.js"></script><script src="/js/tools/tocToggle.js" type="module" data-swup-reload-script=""></script><script src="/js/layouts/toc.js" type="module" data-swup-reload-script=""></script><script src="/js/plugins/tabs.js" type="module" data-swup-reload-script=""></script><script src="/js/libs/moment-with-locales.min.js" data-swup-reload-script=""></script><script src="/js/layouts/essays.js" type="module" data-swup-reload-script=""></script></body></html>